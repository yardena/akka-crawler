akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "INFO"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  logger-startup-timeout = 30s

  actor {
    debug {
      # enable function of LoggingReceive, which is to log any received message at DEBUG level
      receive = on
      # enable DEBUG logging of all AutoReceiveMessages (Kill, PoisonPill and the like)
      autoreceive = on
      # enable DEBUG logging of actor lifecycle changes
      lifecycle = on
      # enable DEBUG logging of unhandled messages
      unhandled = on
    }

    serializers {
      kryo = "com.twitter.chill.akka.AkkaSerializer"
    }

    serialization-bindings {
      "me.yardena.crawler.PageCache$Message$PageEntry" = kryo
    }

  }

  extensions = [akka.persistence.Persistence]

  persistence {

    journal {
      plugin = "akka.persistence.journal.leveldb"
      auto-start-journals = ["akka.persistence.journal.leveldb"]
      leveldb.dir = "journal"
    }

    snapshot-store {
      plugin = "akka.persistence.snapshot-store.local"
      auto-start-snapshot-stores = ["akka.persistence.snapshot-store.local"]
    }

  }

}

parser-dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 4
  }
  throughput = 1
}

client-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 2
    parallelism-max = 8
    parallelism-factor = 2.0
    throughput = 5
  }
}

crawler {
  max-concurrent-pages = 100
  max-concurrent-pages = ${?MAX_PAGES_IN_FLIGHT}
}

fetcher {
  queue-size = 1000
  queue-size = ${?FETCH_QUEUE_SIZE}

  limit-requests-per-second = 10
}

filer {
  save-dir = "./data"
  save-dir = ${?SAVE_DIR}
}

page-handler {
  max-redirects = 3

  check-modified-after = "1h"
  check-modified-after = ${?CHECK_PAGE_MODIFIED_AFTER}

  processing-timeout = "60s"
  processing-timeout = ${?PAGE_PROCESSING_TIMEOUT}
}

page-cache {
  salt = "All you need is love"
}